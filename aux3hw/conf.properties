# flume -> hdfs

flume_hdfs.sources = src1
flume_hdfs.channels = c1 c2
flume_hdfs.sinks = s1 s2

flume_hdfs.sources.src1.type = org.apache.flume.source.kafka.KafkaSource
flume_hdfs.sources.src1.zookeeperConnect = 10.23.15.123:2181
flume_hdfs.sources.src1.topic = logshw

flume_hdfs.sinks.s2.type = hive
flume_hdfs.sinks.s2.hive.metastore = thirft://10.23.15.123:9083
flume_hdfs.sinks.s2.hive.database = default
flume_hdfs.sinks.s2.hive.table = stream_logs
flume_hdfs.sinks.s2.batchSize = 10
flume_hdfs.sinks.s2.serializer = DELIMITED
flume_hdfs.sinks.s2.serializer.delimiter = "\t"
flume_hdfs.sinks.s2.serializer.serdeSeparator = '\t'
flume_hdfs.sinks.s2.serializer.fieldnames = bid_id,time,i_pin_you_id,user_agent,ip,region,city,add_exchange,domain,url,anonimous_url_id,ad_slot_id,ad_slot_width,ad_slot_height,ad_slot_visibility,ad_slot_format,paying_price,creative_id,bidding_price,advertiser_id,user_tags,stream_id,tags

flume_hdfs.sinks.s1.type = hdfs
flume_hdfs.sinks.s1.hdfs.path = hdfs:////tmp/hw/data_destination
flume_hdfs.sinks.s1.hdfs.writeFormat = Text
flume_hdfs.sinks.s1.hdfs.fileType = DataStream

flume_hdfs.sources.src1.interceptors = i1
flume_hdfs.sources.src1.interceptors.i1.type = com.epam.TagInterceptor$Builder
flume_hdfs.sources.src1.interceptors.i1.tagsPath = hdfs:////tmp/hw/tags.txt

flume_hdfs.sources.src1.selector.type = multiplexing
flume_hdfs.sources.src1.selector.header = has_tags
flume_hdfs.sources.src1.selector.mapping.true = c2
flume_hdfs.sources.src1.selector.mapping.false = c1

flume_hdfs.channels.c1.type = memory
flume_hdfs.channels.c1.capacity = 1000

flume_hdfs.channels.c2.type = memory
flume_hdfs.channels.c2.capacity = 1000

flume_hdfs.sources.src1.channels = c1 c2
flume_hdfs.sinks.s1.channel = c1
flume_hdfs.sinks.s2.channel = c2